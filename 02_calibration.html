
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Algorithmic Fairness &#8212; Recidivism Case Study</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '02_calibration';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Fairness by Gender" href="03_fairness.html" />
    <link rel="prev" title="Predicting Crime" href="01_classification.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">Recidivism Case Study</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Recidivism Case Study
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_classification.html">Predicting Crime</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Algorithmic Fairness</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_fairness.html">Fairness by Gender</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/AllenDowney/RecidivismCaseStudy" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/02_calibration.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Algorithmic Fairness</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data">Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-bias">Data bias</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-response">The Response</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calibration">Calibration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrices-and-metrics">Matrices and Metrics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-would-it-take">What Would It Take?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#roc-curve">ROC Curve</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concordance">Concordance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion">Discussion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="algorithmic-fairness">
<h1>Algorithmic Fairness<a class="headerlink" href="#algorithmic-fairness" title="Link to this heading">#</a></h1>
<p>This is the second in a series of notebooks that make up a <a class="reference external" href="https://allendowney.github.io/RecidivismCaseStudy/">case study on classification and algorithmic fairness</a>.
This case study is part of the <a class="reference external" href="https://allendowney.github.io/ElementsOfDataScience/"><em>Elements of Data Science</em></a> curriculum.
<a class="reference external" href="https://colab.research.google.com/github/AllenDowney/RecidivismCaseStudy/blob/v1/02_calibration.ipynb">Click here to run this notebook on Colab</a>.</p>
<p>In the previous chapter, we replicated the analysis reported in
“Machine Bias”, by Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner, published by ProPublica in May 2016.</p>
<p>After the ProPublica article, the Washington Post published a response by Sam Corbett-Davies, Emma Pierson, Avi Feller and Sharad Goel: “A computer program used for bail and sentencing decisions was labeled biased against Blacks. It’s actually not that clear.”</p>
<p>I encourage you to read both of those articles before you go on.  In this chapter, I explain some of the arguments presented in the Washington Post (WaPo) article, and we will replicate their analysis.</p>
<section id="data">
<h2>Data<a class="headerlink" href="#data" title="Link to this heading">#</a></h2>
<p>The authors of “Machine Bias” published their data and analysis in <a class="reference external" href="https://github.com/propublica/compas-analysis">this repository</a>.
The terms of use for the data <a class="reference external" href="https://www.propublica.org/datastore/terms">are here</a>.  In compliance with those terms, I am not redistributing the data.  The following cell downloads the data file we’ll use directly from their repository.</p>
<div class="cell tag_remove-print docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">download</span><span class="p">(</span>
    <span class="s2">&quot;https://github.com/propublica/compas-analysis/raw/master/compas-scores-two-years.csv&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The following cell reads the data file:</p>
<div class="cell tag_remove-print docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">cp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;compas-scores-two-years.csv&quot;</span><span class="p">)</span>
<span class="n">cp</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(7214, 53)
</pre></div>
</div>
</div>
</div>
<p>The dataset includes 7214 rows, one for each defendant, and 53 columns.</p>
</section>
<section id="data-bias">
<h2>Data bias<a class="headerlink" href="#data-bias" title="Link to this heading">#</a></h2>
<p>[<strong>Note:</strong> I wrote about data bias in the previous notebook, but I am repeating it here in case someone reads this notebook without reading the previous one.]</p>
<p>Systems like COMPAS are trying to predict whether a defendant will <em>commit</em> another crime if released.  But the dataset reports whether a defendant was <em>charged</em> with another crime.</p>
<p>Not everyone who commits a crime gets charged (not even close).  The probability of getting charged for a particular crime depends on the type of crime and location; the presence of witnesses and their willingness to work with police; the decisions of police about where to patrol, what crimes to investigate, and who to arrest; and decisions of prosecutors about who to charge.</p>
<p>It is likely that every one of these factors depends on the race of the defendant.  In this dataset, the prevalence of <em>new charges</em> is higher for Black defendants, but that doesn’t necessarily mean that the prevalence of <em>new crimes</em> is higher.</p>
<p>If the dataset is affected by racial bias in the probability of being charged, prediction algorithms like COMPAS will be biased, too.  In discussions of whether and how these systems should be used in the criminal justice system, this is an important issue.</p>
<p>However, I am going to put it aside <em>for now</em> in order to focus on understanding the arguments posed in the ProPublica article and the metrics they are based on.  For the rest of this notebook I will take the “recidivism rates” in the dataset at face value; but I will try to be clear about that they mean (and don’t mean).</p>
</section>
<section id="the-response">
<h2>The Response<a class="headerlink" href="#the-response" title="Link to this heading">#</a></h2>
<p>The Washington Post article summarizes the ProPublica article and the response from Northpointe, the company that makes COMPAS, like this:</p>
<ul class="simple">
<li><p>ProPublica claims that COMPAS is unfair because “among defendants who ultimately did not reoffend, Blacks were more than twice as likely as Whites to be classified as medium or high risk.”</p></li>
<li><p>Northpointe claims that COMPAS is fair because “scores mean essentially the same thing regardless of the defendant’s race. For example, among defendants who scored a seven on the COMPAS scale, 60 percent of White defendants reoffended, which is nearly identical to the 61 percent of Black defendants who reoffended.”</p></li>
</ul>
<p>So ProPublica and Northpointe are invoking different definitions of “fair”.</p>
<p>In the previous chapter we explored the first definition by computing error rates (false positive and false negative) for White and Black defendants.
In this chapter, we’ll explore the second definition, which is called “calibration”.</p>
</section>
<section id="calibration">
<h2>Calibration<a class="headerlink" href="#calibration" title="Link to this heading">#</a></h2>
<p>The WaPo article includes this figure, which shows “White and Black defendants with the same risk score are roughly equally likely to reoffend.”</p>
<p><img alt="" src="https://raw.githubusercontent.com/AllenDowney/RecidivismCaseStudy/v1/calibration1.png" /></p>
<p>To understand this figure, let’s start by replicating it.</p>
<p>The following function groups defendants by risk score and computes the fraction in each group that were charged with another crime within two years.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calibration_curve</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fraction in each risk group charged with another crime.</span>

<span class="sd">    df: DataFrame</span>

<span class="sd">    returns: Series</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">grouped</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;decile_score&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grouped</span><span class="p">[</span><span class="s2">&quot;two_year_recid&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The following figure shows this calibration curve for all defendants and for White and Black defendants separately.</p>
<div class="cell tag_remove-print docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">rcs_utils</span> <span class="kn">import</span> <span class="n">decorate</span><span class="p">,</span> <span class="n">make_matrix</span><span class="p">,</span> <span class="n">compute_metrics</span><span class="p">,</span> <span class="n">error_rates</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cal_all</span> <span class="o">=</span> <span class="n">calibration_curve</span><span class="p">(</span><span class="n">cp</span><span class="p">)</span>
<span class="n">cal_all</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dotted&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;All defendants&quot;</span><span class="p">)</span>

<span class="n">white</span> <span class="o">=</span> <span class="n">cp</span><span class="p">[</span><span class="s2">&quot;race&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Caucasian&quot;</span>
<span class="n">cal_white</span> <span class="o">=</span> <span class="n">calibration_curve</span><span class="p">(</span><span class="n">cp</span><span class="p">[</span><span class="n">white</span><span class="p">])</span>
<span class="n">cal_white</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;White&quot;</span><span class="p">)</span>

<span class="n">black</span> <span class="o">=</span> <span class="n">cp</span><span class="p">[</span><span class="s2">&quot;race&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;African-American&quot;</span>
<span class="n">cal_black</span> <span class="o">=</span> <span class="n">calibration_curve</span><span class="p">(</span><span class="n">cp</span><span class="p">[</span><span class="n">black</span><span class="p">])</span>
<span class="n">cal_black</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Black&quot;</span><span class="p">)</span>

<span class="n">decorate</span><span class="p">(</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Risk score&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Fraction charged with new crime&quot;</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Recivism vs risk score, grouped by race&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7ab0c6eb4ce7624408ab97a0d19bffbf2e8a42f2145db5423b15863739915329.png" src="_images/7ab0c6eb4ce7624408ab97a0d19bffbf2e8a42f2145db5423b15863739915329.png" />
</div>
</div>
<p>This figure shows that people with higher risk scores are more likely to be charged with a new crime within two years.  In that sense COMPAS works as intended.</p>
<p>Furthermore, the test is equally <strong>calibrated</strong> for Black and White defendants; that is, in each risk group, the rate of recidivism is about the same for both groups.</p>
<p>The WaPo article explains why this is important:</p>
<blockquote>
<div><p>A risk score of seven for Black defendants should mean the same thing as a score of seven for White defendants. Imagine if that were not so, and we systematically assigned Whites higher risk scores than equally risky Black defendants with the goal of mitigating ProPublica’s criticism. We would consider that a violation of the fundamental tenet of equal treatment.</p>
</div></blockquote>
<p>So we want a test that has the same calibration for all groups, and we want a test that has the same error rates for all groups.  But there’s the problem: it is mathematically impossible to be fair by both definitions at the same time.</p>
<p>To see why, let’s go back to the confusion matrix.</p>
</section>
<section id="matrices-and-metrics">
<h2>Matrices and Metrics<a class="headerlink" href="#matrices-and-metrics" title="Link to this heading">#</a></h2>
<p>In the previous chapter, we computed confusion matrices for White and Black defendants.  Here they are again:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">matrix_white</span> <span class="o">=</span> <span class="n">make_matrix</span><span class="p">(</span><span class="n">cp</span><span class="p">[</span><span class="n">white</span><span class="p">])</span>
<span class="n">matrix_white</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pred Positive</th>
      <th>Pred Negative</th>
    </tr>
    <tr>
      <th>Actual</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Positive</th>
      <td>505</td>
      <td>461</td>
    </tr>
    <tr>
      <th>Negative</th>
      <td>349</td>
      <td>1139</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">matrix_black</span> <span class="o">=</span> <span class="n">make_matrix</span><span class="p">(</span><span class="n">cp</span><span class="p">[</span><span class="n">black</span><span class="p">])</span>
<span class="n">matrix_black</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pred Positive</th>
      <th>Pred Negative</th>
    </tr>
    <tr>
      <th>Actual</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Positive</th>
      <td>1369</td>
      <td>532</td>
    </tr>
    <tr>
      <th>Negative</th>
      <td>805</td>
      <td>990</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>And here are the metrics we computed from the confusion matrices:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">metrics_white</span> <span class="o">=</span> <span class="n">compute_metrics</span><span class="p">(</span><span class="n">matrix_white</span><span class="p">,</span> <span class="s2">&quot;White defendants&quot;</span><span class="p">)</span>
<span class="n">metrics_white</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Percent</th>
    </tr>
    <tr>
      <th>White defendants</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>FPR</th>
      <td>23.5</td>
    </tr>
    <tr>
      <th>FNR</th>
      <td>47.7</td>
    </tr>
    <tr>
      <th>PPV</th>
      <td>59.1</td>
    </tr>
    <tr>
      <th>NPV</th>
      <td>71.2</td>
    </tr>
    <tr>
      <th>Prevalence</th>
      <td>39.4</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">metrics_black</span> <span class="o">=</span> <span class="n">compute_metrics</span><span class="p">(</span><span class="n">matrix_black</span><span class="p">,</span> <span class="s2">&quot;Black defendants&quot;</span><span class="p">)</span>
<span class="n">metrics_black</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Percent</th>
    </tr>
    <tr>
      <th>Black defendants</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>FPR</th>
      <td>44.8</td>
    </tr>
    <tr>
      <th>FNR</th>
      <td>28.0</td>
    </tr>
    <tr>
      <th>PPV</th>
      <td>63.0</td>
    </tr>
    <tr>
      <th>NPV</th>
      <td>65.0</td>
    </tr>
    <tr>
      <th>Prevalence</th>
      <td>51.4</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>If we look at the error rates (FPR and FNR), it seems like COMPAS is biased against Black defendants:</p>
<ul class="simple">
<li><p>Their false positive rate is higher (45% vs 23%): among people who <em>will not</em> recidivate, Black defendants are more likely to be classified high risk.</p></li>
<li><p>Their false negative rate is lower (28% vs 48%): among people who <em>will</em> recidivate, Black defendants are less likely to be classified low risk.</p></li>
</ul>
<p>But if we look at the the predictive values (PPV and NPV) it seems like COMPAS is biased in favor of Black defendants:</p>
<ul class="simple">
<li><p>Among people in the <em>high risk group</em>, Black defendants are more likely to be charged with another crime (63% vs 59%).</p></li>
<li><p>Among people in the <em>low risk group</em>, Black defendants are less likely to “survive” two years without another charge (65% vs 71%).</p></li>
</ul>
<p>It seems like we should be able to fix these problems, but it turns out that we can’t.
We’ll see why in the next section.</p>
</section>
<section id="what-would-it-take">
<h2>What Would It Take?<a class="headerlink" href="#what-would-it-take" title="Link to this heading">#</a></h2>
<p>Suppose we want to fix COMPAS so that predictive values are the same for Black and White defendants.  We could do that by using different thresholds for the two groups.
In this section, we’ll figure out what it would take to re-calibrate COMPAS; then we’ll see what effect that would have on predictive values.</p>
<p>The following function loops through possible thresholds, makes the confusion matrix with each threshold, and computes accuracy metrics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sweep_threshold</span><span class="p">(</span><span class="n">cp</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sweep a range of threshold and compute accuracy metrics.</span>

<span class="sd">    cp: DataFrame of COMPAS data</span>

<span class="sd">    returns: DataFrame with one row for each threshold and</span>
<span class="sd">             one column for each metric</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">index</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;FPR&quot;</span><span class="p">,</span> <span class="s2">&quot;FNR&quot;</span><span class="p">,</span> <span class="s2">&quot;PPV&quot;</span><span class="p">,</span> <span class="s2">&quot;NPV&quot;</span><span class="p">,</span> <span class="s2">&quot;Prevalence&quot;</span><span class="p">]</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">index</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">threshold</span> <span class="ow">in</span> <span class="n">index</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">make_matrix</span><span class="p">(</span><span class="n">cp</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="n">compute_metrics</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
        <span class="n">table</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">threshold</span><span class="p">]</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;Percent&quot;</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">table</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s the resulting table for all defendants.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">table_all</span> <span class="o">=</span> <span class="n">sweep_threshold</span><span class="p">(</span><span class="n">cp</span><span class="p">)</span>
<span class="n">table_all</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>FPR</th>
      <th>FNR</th>
      <th>PPV</th>
      <th>NPV</th>
      <th>Prevalence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>100.0</td>
      <td>0.0</td>
      <td>45.1</td>
      <td>NaN</td>
      <td>45.1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>71.4</td>
      <td>9.5</td>
      <td>51.0</td>
      <td>78.6</td>
      <td>45.1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>55.1</td>
      <td>18.5</td>
      <td>54.8</td>
      <td>74.8</td>
      <td>45.1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>43.3</td>
      <td>27.1</td>
      <td>58.0</td>
      <td>71.8</td>
      <td>45.1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>32.3</td>
      <td>37.4</td>
      <td>61.4</td>
      <td>68.8</td>
      <td>45.1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The following figure shows error rates as a function of threshold.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">table_all</span><span class="p">[</span><span class="s2">&quot;FPR&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;C2&quot;</span><span class="p">)</span>
<span class="n">table_all</span><span class="p">[</span><span class="s2">&quot;FNR&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;C4&quot;</span><span class="p">)</span>

<span class="n">decorate</span><span class="p">(</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Threshold&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Percent&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Error rates for a range of thresholds&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7cacc8552280740c58c4ae6a4d25b001a75064035f4b8470a50328c7aee198ed.png" src="_images/7cacc8552280740c58c4ae6a4d25b001a75064035f4b8470a50328c7aee198ed.png" />
</div>
</div>
<p>When the threshold is low, almost everyone is in the high risk group; in that case:</p>
<ul class="simple">
<li><p>FNR is low because most recidivists are in the high risk group, but</p></li>
<li><p>FPR is high because most non-recidivists are <em>also</em> in the high risk group.</p></li>
</ul>
<p>When the threshold is high, almost everyone is in the low risk group, and the metrics are the other way around:</p>
<ul class="simple">
<li><p>FPR is low because most non-recidivists are in the low risk group, but</p></li>
<li><p>FNR is high because most recidivists are <em>also</em> in the low risk group.</p></li>
</ul>
<p>The following figure shows predictive values for a range of thresholds.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">table_all</span><span class="p">[</span><span class="s2">&quot;PPV&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">)</span>
<span class="n">table_all</span><span class="p">[</span><span class="s2">&quot;NPV&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">)</span>

<span class="n">decorate</span><span class="p">(</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Threshold&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Percent&quot;</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Predictive values for a range of thresholds&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/33c7b0c48529c7ccb85e6949f0cfce5ba764783b92215254e623914ec237cf7b.png" src="_images/33c7b0c48529c7ccb85e6949f0cfce5ba764783b92215254e623914ec237cf7b.png" />
</div>
</div>
<p>When the threshold is too low, PPV is low.  When the threshold is too high, NPV is low.</p>
<p>Now let’s compute tables for Black and White defendants separately.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">table_white</span> <span class="o">=</span> <span class="n">sweep_threshold</span><span class="p">(</span><span class="n">cp</span><span class="p">[</span><span class="n">white</span><span class="p">])</span>
<span class="n">table_black</span> <span class="o">=</span> <span class="n">sweep_threshold</span><span class="p">(</span><span class="n">cp</span><span class="p">[</span><span class="n">black</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We’ll use the following function to interpolate columns in the table; that is, for a given threshold I can compute the corresponding metric.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.interpolate</span> <span class="kn">import</span> <span class="n">interp1d</span>


<span class="k">def</span> <span class="nf">interpolate</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Evaluate a function at a value.</span>

<span class="sd">    series: Series</span>
<span class="sd">    value: number</span>
<span class="sd">    options: passed to interp1d (default is linear interp)</span>

<span class="sd">    returns: number</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">interp</span> <span class="o">=</span> <span class="n">interp1d</span><span class="p">(</span><span class="n">series</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">series</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">interp</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The following function goes the other way: it estimates the threshold where a column passes through a given metric.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">crossing</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Find where a function crosses a value.</span>

<span class="sd">    series: Series</span>
<span class="sd">    value: number</span>
<span class="sd">    options: passed to interp1d (default is linear interp)</span>

<span class="sd">    returns: number</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">interp</span> <span class="o">=</span> <span class="n">interp1d</span><span class="p">(</span><span class="n">series</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">series</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">interp</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can use <code class="docutils literal notranslate"><span class="pre">crossing</span></code> to calibrate the test for White defendants; that is, we can compute the threshold that would make the error rates for White defendants the same as for the general population.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">matrix_all</span> <span class="o">=</span> <span class="n">make_matrix</span><span class="p">(</span><span class="n">cp</span><span class="p">)</span>
<span class="n">fpr</span><span class="p">,</span> <span class="n">fnr</span> <span class="o">=</span> <span class="n">error_rates</span><span class="p">(</span><span class="n">matrix_all</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">crossing</span><span class="p">(</span><span class="n">table_white</span><span class="p">[</span><span class="s2">&quot;FPR&quot;</span><span class="p">],</span> <span class="n">fpr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array(3.23050171)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">crossing</span><span class="p">(</span><span class="n">table_white</span><span class="p">[</span><span class="s2">&quot;FNR&quot;</span><span class="p">],</span> <span class="n">fnr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array(3.11998938)
</pre></div>
</div>
</div>
</div>
<p>With a threshold near 3.2, White defendants would have the same error rates as the general population.
Now let’s do the same computation for Black defendants.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">crossing</span><span class="p">(</span><span class="n">table_black</span><span class="p">[</span><span class="s2">&quot;FPR&quot;</span><span class="p">],</span> <span class="n">fpr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array(5.20752868)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">crossing</span><span class="p">(</span><span class="n">table_black</span><span class="p">[</span><span class="s2">&quot;FNR&quot;</span><span class="p">],</span> <span class="n">fnr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array(5.01788384)
</pre></div>
</div>
</div>
</div>
<p>To get the same error rates for Black and White defendants, we would need different thresholds: about 5.1 compared to 3.2.
At those levels, the predictive values are substantially different.
Here’s PPV with different thresholds for each group:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">interpolate</span><span class="p">(</span><span class="n">table_white</span><span class="p">[</span><span class="s2">&quot;PPV&quot;</span><span class="p">],</span> <span class="mf">3.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array(55.26)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">interpolate</span><span class="p">(</span><span class="n">table_black</span><span class="p">[</span><span class="s2">&quot;PPV&quot;</span><span class="p">],</span> <span class="mf">5.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array(66.17)
</pre></div>
</div>
</div>
</div>
<p>With equal error rates, we get different PPV:</p>
<ul class="simple">
<li><p>Among White defendants in the high risk group, about 55% would recidivate.</p></li>
<li><p>Among Black defendants in the high risk group, about 66% would recidivate.</p></li>
</ul>
<p>Here’s NPV with different thresholds for each group:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">interpolate</span><span class="p">(</span><span class="n">table_white</span><span class="p">[</span><span class="s2">&quot;NPV&quot;</span><span class="p">],</span> <span class="mf">3.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array(73.04)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">interpolate</span><span class="p">(</span><span class="n">table_black</span><span class="p">[</span><span class="s2">&quot;NPV&quot;</span><span class="p">],</span> <span class="mf">5.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array(62.19)
</pre></div>
</div>
</div>
</div>
<p>With equal error rates, the NPVs are substantially different:</p>
<ul class="simple">
<li><p>Among White defendants in the low risk group, 73% went two years without another charge.</p></li>
<li><p>Among Black defendants in the low risk group, 62% went two years without another charge.</p></li>
</ul>
<p>To summarize, if the test is calibrated in terms of error rates, it is not calibrated in terms of predictive values.</p>
<ul class="simple">
<li><p>If we make the error rates more equal, we make the predictive values more unfair, and</p></li>
<li><p>If we make the predictive values more equal, we make the error rates more unfair.</p></li>
</ul>
<p>Fundamentally, the problem is that the prevalence of recidivism is different in the two groups: about 39% of White defendants were charged with another crime within two years, compared to 51% of Black defendants.
As long as that’s the case (for any two groups) the predictive values and error rates can’t be “fair” at the same time.</p>
<p>That’s the argument the Washington Post article presented.
In the next section, we’ll take the argument one step farther by introducing one more metric, the area under the ROC curve.</p>
</section>
<section id="roc-curve">
<h2>ROC Curve<a class="headerlink" href="#roc-curve" title="Link to this heading">#</a></h2>
<p>In the previous section we plotted various metrics as as function of threshold.  Another common and useful way to visualize these results is to plot sensitivity versus false positive rate (FPR).  For historical reasons, the result is called a <strong>receiver operating characteristic (ROC) curve</strong>.</p>
<p>The following function plots the ROC curve:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>


<span class="k">def</span> <span class="nf">plot_roc</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot the ROC curve.</span>

<span class="sd">    table: DataFrame of metrics as a function of</span>
<span class="sd">           classification threshold</span>
<span class="sd">    options: passed to plot</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
    <span class="n">sens</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">-</span> <span class="n">table</span><span class="p">[</span><span class="s2">&quot;FNR&quot;</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">table</span><span class="p">[</span><span class="s2">&quot;FPR&quot;</span><span class="p">],</span> <span class="n">sens</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">)</span>
    <span class="n">decorate</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;FPR&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Sensitivity (1-FNR)&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;ROC curve&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s the ROC curve for all defendants.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_roc</span><span class="p">(</span><span class="n">table_all</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C2&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;All defendants&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/bb5b7a5b42b7f79a6987077702d67a63ee2b468ab05bc77a0171d4b1803b363c.png" src="_images/bb5b7a5b42b7f79a6987077702d67a63ee2b468ab05bc77a0171d4b1803b363c.png" />
</div>
</div>
<p>The green line is the ROC curve.  The gray dotted line shows the identity line for comparison.</p>
<p>An ideal test would have high sensitivity for all values of FPR, but in reality there is almost always a trade-off:</p>
<ul class="simple">
<li><p>When FPR is low, sensitivity is low.</p></li>
<li><p>In order to get more sensitivity, we have to accept a higher FPR.</p></li>
</ul>
<p>The ROC curve tells us how much sensitivity we get for a given FPR or, the other way around, how much FPR we have to accept to achieve a given sensitivity.
The following figure shows the ROC curves for White and Black defendants.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_roc</span><span class="p">(</span><span class="n">table_white</span><span class="p">)</span>
<span class="n">plot_roc</span><span class="p">(</span><span class="n">table_black</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/61529edcf17f2096206034224746280e754499a7685bb6f3435e36cda01c74aa.png" src="_images/61529edcf17f2096206034224746280e754499a7685bb6f3435e36cda01c74aa.png" />
</div>
</div>
<p>The ROC curves are similar for the two groups, which shows that we can achieve nearly the same error rates (FPR and FNR) for the two groups, as we did in the previous section.
It also shows that the test has nearly the same “concordance” for both groups, which I explain in the next section.</p>
</section>
<section id="concordance">
<h2>Concordance<a class="headerlink" href="#concordance" title="Link to this heading">#</a></h2>
<p>The authors of the ProPublica article published a supplementary article,
“How We Analyzed the COMPAS Recidivism Algorithm”, which describes their analysis in more detail (see <a class="reference external" href="https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm">https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm</a>).</p>
<p>As another metric of accuracy, they estimate <strong>concordance</strong>, which they describe like this:</p>
<blockquote>
<div><p>Overall, [COMPAS has] a concordance score of 63.6 percent.  That means for any randomly selected pair of defendants in the sample, the COMPAS system can accurately rank their recidivism risk 63.6 percent of the time (e.g. if one person of the pair recidivates, that pair will count as a successful match if that person also had a higher score). In its study, Northpointe reported a slightly higher concordance: 68 percent.</p>
</div></blockquote>
<p>They explain:</p>
<blockquote>
<div><p>[These estimates] are lower than what Northpointe describes as a threshold for reliability. “A rule of thumb according to several recent articles is that [concordances] of .70 or above typically indicate satisfactory predictive accuracy, and measures between .60 and .70 suggest low to moderate predictive accuracy,” the company says in its study.</p>
</div></blockquote>
<p>There are several ways to compute concordance, but one of the simplest is to compute the area under the ROC curve, which is why concordance is also called the <strong>area under the curve</strong> or AUC.
Since we’ve already computed the ROC, we can use Simpson’s rule to estimate the area under the curve (see <a class="reference external" href="https://en.wikipedia.org/wiki/Simpsons_rule">https://en.wikipedia.org/wiki/Simpsons_rule</a>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.integrate</span> <span class="kn">import</span> <span class="n">simpson</span>


<span class="k">def</span> <span class="nf">compute_auc</span><span class="p">(</span><span class="n">table</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the area under the ROC curve.&quot;&quot;&quot;</span>
    <span class="n">y</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">-</span> <span class="n">table</span><span class="p">[</span><span class="s2">&quot;FNR&quot;</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">table</span><span class="p">[</span><span class="s2">&quot;FPR&quot;</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">sort_index</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">/</span> <span class="mi">100</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sort_index</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">/</span> <span class="mi">100</span>
    <span class="k">return</span> <span class="n">simpson</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The concordance for all respondents is about 70%.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">compute_auc</span><span class="p">(</span><span class="n">table_all</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7061166121516749
</pre></div>
</div>
</div>
</div>
<p>For the subgroups it is slightly lower, but also near 70%.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">compute_auc</span><span class="p">(</span><span class="n">table_white</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.6996145234049567
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">compute_auc</span><span class="p">(</span><span class="n">table_black</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.6946519102148443
</pre></div>
</div>
</div>
</div>
<p>Different ways of computing concordance handle ties differently, which is probably why we, ProPublica, and Northpointe get somewhat different estimates.
But qualitatively they all tell the same story; as a binary classifier, COMPAS is only moderately accurate. However, it seems to be equally accurate, by this metric, for White and Black defendants.</p>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>In this chapter, we replicated the analysis reported in the Washington Post article and confirmed two of the arguments they presented:</p>
<ol class="arabic simple">
<li><p>COMPAS is calibrated in the sense that White and Black defendants with the same risk score have almost the same probability of being charged with another crime.  This implies that it has roughly the same predictive value for both groups.</p></li>
<li><p>It is impossible for COMPAS to have the same predictive values for both groups and the same error rates at the same time.</p></li>
</ol>
<p>And we showed:</p>
<ul class="simple">
<li><p>If you design a test to achieve equal predictive value across groups with different prevalence, you will find that error rates differ.  Specifically, false positive rates will be higher in groups with higher recividism.</p></li>
<li><p>If you design a test to achieve equal error rates across groups, you will find that predictive values differ.  Specifically, positive predictive value will be lower in groups with lower rates of recidivism.</p></li>
</ul>
<p>Finally, we derived the ROC curve and computed AUC, which shows that COMPAS has nearly the same concordance for White and Black defendants.</p>
<p>In <a class="reference external" href="https://colab.research.google.com/github/AllenDowney/RecidivismCaseStudy/blob/v1/03_fairness.ipynb">the next notebook</a> we’ll apply the same analysis to evaluate the performance of COMPAS for male and female defendants.  We’ll find that COMPAS is unfair to women, but in a way that’s opposite what we have seen so far: the error rates are about the same for both groups, but the predictive values are substantially different.</p>
</section>
<section id="discussion">
<h2>Discussion<a class="headerlink" href="#discussion" title="Link to this heading">#</a></h2>
<p>If it is impossible to make a classification algorithm fair for all groups, what should we do?</p>
<p>It might be tempting to forbid algorithms like COMPAS in the criminal justice system, but unfortunately, that doesn’t solve the problem.
The conclusions we reached in this case study apply to human decision-makers as well, with the additional problem that humans are more unpredictable than algorithms, and can be more biased.
As long as we have to make decisions about bail, sentencing, and parole, we will need data and algorithms to inform those decisions, regardless of whether the algorithms are run by humans or machines.</p>
<p>I don’t have a solution to these problems, but I will suggest two guidelines: the data we use should be unbiased, and the algorithms should be transparent.</p>
<p>I discussed the problem of data bias in the previous chapter. In this example, we used data about additional <em>charges</em> as a measure of additional <em>crimes</em>. But not everyone who commits a crime gets charged. If one group is more likely than another to be charged with a crime, the algorithm will be unfair, no matter how it is calibrated.</p>
<p>Of course we should use unbiased data if we can, but if that’s not possible, sometimes we can do as well if we <em>know</em> the data is biased, in what directions, and by how much.
So one thing we can do to make algorithms more fair is to quantify biases in the data we use and compensate for them.</p>
<p>Another thing we can do is make systems like COMPAS more transparent; that is, we should know how they work, what factors they take into account, and what they ignore.
Algorithms intended to serve the public interest should be the subject of public discussion, not the trade secrets of a private company.</p>
<p>The use of algorithms in criminal justice, and in many other domains that profoundly affect people’s lives, is relatively new.
It raises difficult questions about fairness that we have only begun to recognize.</p>
<p>The goal of this case study is to help us measure the accuracy of these algorithms and quantify their fairness. I hope it will contribute to the ongoing debate as we address the many challenges of the criminal justice system and the use of algorithms in our lives.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="01_classification.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Predicting Crime</p>
      </div>
    </a>
    <a class="right-next"
       href="03_fairness.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Fairness by Gender</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data">Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-bias">Data bias</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-response">The Response</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calibration">Calibration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrices-and-metrics">Matrices and Metrics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-would-it-take">What Would It Take?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#roc-curve">ROC Curve</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concordance">Concordance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion">Discussion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Allen B. Downey
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>